本文发现一些句子中可能隐式地含有一些关系，提取出他们对于RTE来说是个挑战，故设计了关系指导的注意力机制(RGAM)，将关系作为先验知识而非分类的标签，从而去指导主语寻找对应宾语。即：首先提取出可能的主语，在受指导的目标关系作用下寻找对应宾语。本方法不仅可以学习多重依赖，也适合隐式关系的提取，以及处理重叠问题。数据集：NYT和WebNLG。 (IJCNN2021 CCF-C)

<!--more-->
<hr>
论文全名：A Relation-Guided Attention Mechanism for Relational Triple Extraction
<hr>

# Model Details:
- 猜测本文的改进点是大部分基于论文CasRel的，故不分析具体的细节，可与论文CasRel对照其结构图来看起改进点所在。<a href="http://aca.abelcse.cn/index.php/archives/80/">论文阅读(CasRel)</a>

### 实体标记策略：
- 本文的标记策略与论文CasRel一样，就是设计出上下两行进行二元的标记，分别表示开始和结束，若其概率超过某一阈值，则标记为1，否则都为0；
- 通过最近匹配原则，挨的最近的`开始`和`结束`位置所对应的那些tokens就是被标记出来的主语(宾语)。

### 编码器：
- 本文使用了BERT-Base-Cased作为Encoder，他们发现使用随机初始化的BERT效果不好，所以后面的实验结果是预训练后的，此外由于该任务每次输入的是单个句子而非句子对，所以并没有Segment Embedding；
- 本文的BERT作用主要是为模型提供关于关系的先验知识；个人感觉本文主要的贡献在于BERT。

### Subject & Object Decoder:
- 按照实体标记策略，得到了一组主语集合S，其操作与CasRel基本一致；
- 宾语解码上有所不同，按照文章所说，其不同在于他们是：

 $$f(s,r)->o \,\,\,but\,\,\,  f_r(s)->o$$

- 宾语解码时分了三部分，`主语表示模块`对每个主语的开始与结束位置的token求平均值，以得到相应的表示；`关系指导注意力模块`利用一个NLP工具*bert-as-service*得到每种关系的预训练向量表示，以及一些包括注意力机制在内的操作得到关系指导的表示；最后的`融合模块`利用主语信息、关系指导的信息来得到更好的一种token表示，最终实现宾语的标记；
- 损失函数就是联合的两个交叉熵进行；
- 总体结构图如下：

[![model arch](https://s2.loli.net/2022/01/21/kIPL5Wbjzvq62in.png "model arch")](https://s2.loli.net/2022/01/21/kIPL5Wbjzvq62in.png "model arch")

<hr>

# Experiment:
### 数据集：
- NYT24和WebNLG，但是本文把WebNLG的预定义关系数量写成了246，因为CasRel也误写为了246，实际上2020年的论文TPlinker已经纠正了该问题(即标准WebNLG为216个)；
### 实验结果
- 文章中规中矩的进行了三个指标Precision、Recall和F1-Score的比对，单独在NYT上抽取了很多隐式关系的句子进行比较，以及在实体重叠问题和复杂句子上进行了比较，确实与他比较的论文相比都是最优；
- 最主要的实验结果表如下：

[![main experiment result](https://s2.loli.net/2022/01/21/4BVj3gxn7UECe5H.png "main experiment result")](https://s2.loli.net/2022/01/21/4BVj3gxn7UECe5H.png "main experiment result")

- 本文的消融实验希望证明自己的关系指导的注意力起了很大作用，虽然确实有一定的作用，但是个人任务主要贡献度还是在BERT所提供的先验知识，他们只是起了锦上添花的作用。

<hr>

# Conclusion:
- 文章主要的创新点在于：利用关系指导的注意力机制捕获了细粒度的语义关系，得到更准确的宾语；
- 本文的贡献在于：1). 利用BERT作为关系的先验知识，由于BERT是基于大规模语料训练的，所以隐式的关系发现应该归功于BERT；2). 借鉴CasRel设计的标注策略简单高效的完成了实体提取任务，而这种利用主语、关系指导的表示去寻找宾语的措施也有效的缓解了实体重叠问题。
- 本文的缺点在于(站在现在的视角评价)：BERT的贡献更大，对自己创新点和贡献的逻辑论证不够令人信服。

<hr>

***注:*** **本文的所有分析均由个人得出，仅供个人参考，涉及主观评价的部分请自行判断，若有任何误读请留言告知**

<hr>